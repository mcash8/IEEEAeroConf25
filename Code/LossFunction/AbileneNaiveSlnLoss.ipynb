{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcash8/IEEEAeroConf25/blob/main/Code/LossFunction/AbileneNaiveSlnLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gurobipy  # install gurobipy, if not already installed"
      ],
      "metadata": {
        "id": "3xdgP3x5TL5k",
        "outputId": "eacce9c0-e662-48de-a5c5-d9f5afc2d173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gurobipy\n",
            "  Downloading gurobipy-11.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Downloading gurobipy-11.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gurobipy\n",
            "Successfully installed gurobipy-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "wPZoQyvu6xuI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Functions\n"
      ],
      "metadata": {
        "id": "MxbnFUemgwmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Utility Functions\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from matplotlib import pyplot as plt\n",
        "import gurobipy as gb\n",
        "import random\n",
        "import itertools as it\n",
        "\n",
        "\n",
        "'''\n",
        "    Build a fully connected network\n",
        "'''\n",
        "\n",
        "def fully_connected_network(num_nodes = 4, capacity = 100):\n",
        "\n",
        "     # Create a directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes\n",
        "    G.add_nodes_from(range(num_nodes))\n",
        "\n",
        "    # Add edges with capacity\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(num_nodes):\n",
        "            if i != j:\n",
        "                G.add_edge(i, j, capacity=capacity)\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "'''\n",
        "    Build a star network\n",
        "'''\n",
        "\n",
        "def star_network(num_nodes = 4, capacity = 100):\n",
        "    if num_nodes < 2:\n",
        "        raise ValueError(\"The number of nodes must be at least 2 to create a star network.\")\n",
        "\n",
        "    # Create a directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes\n",
        "    G.add_nodes_from(range(num_nodes))\n",
        "\n",
        "    # The central node is node 0\n",
        "    central_node = 0\n",
        "\n",
        "    # Add edges from the central node to all other nodes\n",
        "    for node in range(1, num_nodes):\n",
        "        G.add_edge(central_node, node, capacity=capacity)\n",
        "        G.add_edge(node, central_node, capacity=capacity)\n",
        "\n",
        "    return G\n",
        "\n",
        "'''\n",
        "    Build a line network\n",
        "'''\n",
        "\n",
        "def line_network(num_nodes = 4, capacity = 100):\n",
        "\n",
        "    # Create an undirected graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes\n",
        "    G.add_nodes_from(range(num_nodes))\n",
        "\n",
        "    # Add edges with specified capacity\n",
        "    for i in range(num_nodes - 1):\n",
        "        # Connect node i to node i+1\n",
        "        G.add_edge(i, i + 1, capacity=capacity)\n",
        "        G.add_edge(i+1, i, capacity=capacity)\n",
        "\n",
        "    return G\n",
        "\n",
        "'''\n",
        "    Cosine Similarity Function\n",
        "'''\n",
        "\n",
        "def cosine_similarity(A, B):\n",
        "\n",
        "    # Flatten the matrices\n",
        "    vec1 = A.flatten()\n",
        "    vec2 = B.flatten()\n",
        "\n",
        "    # Compute dot product\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "\n",
        "    # Compute norms\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = dot_product / (norm1 * norm2)\n",
        "\n",
        "    return 1-similarity\n",
        "\n",
        "'''\n",
        "    Build the Abilene Topology\n",
        "'''\n",
        "def abilene_topo():\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    G.add_nodes_from(range(12))\n",
        "\n",
        "    #add edges\n",
        "    with open('data/topo.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "\n",
        "            # Extract node1, node2, and capacity\n",
        "            node1 = int(parts[0])\n",
        "            node2 = int(parts[1])\n",
        "            capacity = float(parts[2])\n",
        "\n",
        "            # Add the edges with the specified capacity\n",
        "            G.add_edge(node1, node2, capacity=capacity)\n",
        "            G.add_edge(node2, node1, capacity=capacity)\n",
        "\n",
        "    return G\n",
        "\n",
        "def abilene_topo_lossfn():\n",
        "        '''\n",
        "            If we use the original topology the MLU will be very small and differentiating is difficult.\n",
        "            This topology scales the capacity values to be between 1 and 0.25.\n",
        "        '''\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        G.add_nodes_from(range(12))\n",
        "\n",
        "        #add edges\n",
        "        with open('topo.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "\n",
        "                # Extract node1, node2, and capacity\n",
        "                node1 = int(parts[0])\n",
        "                node2 = int(parts[1])\n",
        "\n",
        "                if (int(parts[2]) == 9920000):\n",
        "                    capacity = 10\n",
        "                elif (int(parts[2]) == 2480000):\n",
        "                    capacity = 2\n",
        "\n",
        "                # Add the edges with the specified capacity\n",
        "                G.add_edge(node1, node2, capacity=capacity)\n",
        "                G.add_edge(node2, node1, capacity=capacity)\n",
        "\n",
        "        return G\n",
        "\n",
        "\n",
        "'''\n",
        "    Functions for MCF\n",
        "'''\n",
        "\n",
        "def cartesian_product(*arrays):\n",
        "    la = len(arrays)\n",
        "    dtype = np.result_type(*arrays)\n",
        "    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n",
        "    for i, a in enumerate(np.ix_(*arrays)):\n",
        "        arr[...,i] = a\n",
        "    return arr.reshape(-1, la)\n",
        "\n",
        "def  getCosts(G):\n",
        "    '''\n",
        "        Get cost for edges in G\n",
        "    '''\n",
        "    has_cost_attribute = all('cost' in G.edges[edge] for edge in G.edges())\n",
        "\n",
        "    if has_cost_attribute:\n",
        "        cost = nx.get_edge_attributes(G, 'cost')\n",
        "    else:\n",
        "        # If costs aren't specified, make uniform.\n",
        "        nx.set_edge_attributes(G, name='cost', values=1)\n",
        "        cost = nx.get_edge_attributes(G, 'cost')\n",
        "\n",
        "    return cost\n",
        "\n",
        "def getCapacity(G):\n",
        "    '''\n",
        "        Get capacity of edges\n",
        "    '''\n",
        "    return nx.get_edge_attributes(G, 'capacity')\n",
        "\n",
        "def getLoadandFlowVars(m, V, arcs, cost):\n",
        "\n",
        "    f = m.addVars(V, V, arcs, obj=cost, name='flow')\n",
        "    l = m.addVars(arcs, lb=0.0, name='tot_traf_across_link')\n",
        "\n",
        "    return f, l\n",
        "\n",
        "def getLinkUtilization(m, l, f, arcs):\n",
        "\n",
        "    '''Link util = sum over flows for each od pair '''\n",
        "\n",
        "    # Link utilization = sum of flow\n",
        "    m.addConstrs((l[i, j] == f.sum('*', '*', i, j) for i, j in arcs), 'l_sum_traf',)\n",
        "\n",
        "    return\n",
        "\n",
        "def getCapacityConstraint(m, l, capacity, arcs):\n",
        "    '''Link utilzation can not exceed link capacity'''\n",
        "    # Add capacity constraints\n",
        "    m.addConstrs(\n",
        "        (l[i, j] <= capacity[i,j] for i, j in arcs),\n",
        "        'traf_below_cap',\n",
        "        )\n",
        "\n",
        "    return\n",
        "\n",
        "def getFlowConservationConstraint(m, D, V, f):\n",
        "    ''' No flow gets left behind'''\n",
        "\n",
        "    for s, t, u in cartesian_product(V, V, V):\n",
        "        d = D[int(s-1), int(t-1)]\n",
        "\n",
        "        if u==s:\n",
        "            m.addConstr(f.sum(s, t, u, '*')-f.sum(s, t, '*', u)==d, 'conserv')\n",
        "        elif u==t:\n",
        "            m.addConstr(f.sum(s, t, u, '*')-f.sum(s, t, '*', u)==-d, 'conserv')\n",
        "        else:\n",
        "            m.addConstr(f.sum(s, t, u, '*')-f.sum(s, t, '*', u)==0, 'conserv')\n",
        "\n",
        "    return\n",
        "\n",
        "def MinMaxLinkUtil(G,D, verbose = False):\n",
        "    params = {\n",
        "    \"WLSACCESSID\": 'a5796427-505f-41e2-8e78-716fe8f5d2c0',\n",
        "    \"WLSSECRET\": 'ebe52f9b-4daa-436a-92c1-1f69a7ef642b',\n",
        "    \"LICENSEID\": 2508231,\n",
        "    \"OutputFlag\": 0,\n",
        "    }\n",
        "\n",
        "    env = gb.Env(params=params)\n",
        "\n",
        "    # Create instance of optimizer model\n",
        "    m = gb.Model('netflow', env=env)\n",
        "    V = np.array([i for i in G.nodes()])\n",
        "\n",
        "    verboseprint = print\n",
        "    if not verbose:\n",
        "        verboseprint = lambda *a: None\n",
        "        m.setParam('OutputFlag', False )\n",
        "        m.setParam('LogToConsole', False )\n",
        "\n",
        "    # Get link costs and capacity\n",
        "    cost = getCosts(G)\n",
        "    cap = getCapacity(G)\n",
        "\n",
        "    arcs, capacity = gb.multidict(cap)\n",
        "\n",
        "    # Create flow and load variables\n",
        "    f, l = getLoadandFlowVars(m, V, arcs, cost)\n",
        "\n",
        "    # Add link utilization constraints\n",
        "    getLinkUtilization(m, l, f, arcs)\n",
        "\n",
        "    # Add capacity constraints\n",
        "    getCapacityConstraint(m, l, capacity, arcs)\n",
        "\n",
        "    # Add flow conservation constraints\n",
        "    getFlowConservationConstraint(m, D, V, f)\n",
        "\n",
        "    # Set objective to max-link utilization (congestion)\n",
        "    max_cong = m.addVar(name='congestion')\n",
        "    m.addConstrs(((cost[i,j]*l[i, j])/capacity[i,j]<= max_cong for i, j in arcs))\n",
        "    m.setObjective(max_cong, gb.GRB.MINIMIZE)\n",
        "\n",
        "    # Compute optimal solution\n",
        "    m.optimize()\n",
        "\n",
        "    # Print solution\n",
        "    if m.status == gb.GRB.Status.OPTIMAL:\n",
        "        f_sol = m.getAttr('x', f)\n",
        "        l_sol = m.getAttr('x', l)\n",
        "        m_cong = float(max_cong.x)\n",
        "\n",
        "        verboseprint('\\nOptimal traffic flows.')\n",
        "        verboseprint('f_{i -> j}(s, t) denotes amount of traffic from source'\n",
        "                     ' s to destination t that goes through link (i, j) in E.')\n",
        "\n",
        "        for s, t in cartesian_product(V, V):\n",
        "            for i,j in arcs:\n",
        "                p = f_sol[s, t, i, j]\n",
        "                if p > 0:\n",
        "                    verboseprint('f_{%s -> %s}(%s, %s): %g bytes.'\n",
        "                                  % (i, j, s, t, p))\n",
        "\n",
        "        verboseprint('\\nTotal traffic through link.')\n",
        "        verboseprint('l(i, j) denotes the total amount of traffic that passes'\n",
        "                     ' through edge (i, j).'\n",
        "        )\n",
        "\n",
        "        for i, j in arcs:\n",
        "            p = l_sol[i, j]\n",
        "            if p > 0:\n",
        "                verboseprint('%s -> %s: %g bytes.' % (i, j, p))\n",
        "\n",
        "        verboseprint('\\nMaximum weighted link utilization (or congestion):',\n",
        "                     format(m_cong, '.4f')\n",
        "        )\n",
        "\n",
        "\n",
        "        return m_cong\n",
        "    else:\n",
        "        return\n",
        "\n",
        "\n",
        "'''\n",
        "    Functions for data processing\n",
        "'''\n",
        "\n",
        "def create_dataset(dataset, window_size):\n",
        "    dataX, dataY = [], []\n",
        "\n",
        "    for i in range(len(dataset)-window_size):\n",
        "        a = dataset[i:i+window_size, :]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + window_size, :])\n",
        "\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "def train_test_split(dataset, train_ratio):\n",
        "\n",
        "    train_size = int(len(dataset) * train_ratio)\n",
        "\n",
        "    train_data = dataset[0:train_size,:]\n",
        "    test_data = dataset[train_size:len(dataset),:]\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "def normalize_matrix(scaler, dataset):\n",
        "\n",
        "    dataset_norm = np.zeros(dataset.shape)\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        row = np.reshape(dataset[i, :], (dataset.shape[1],1))\n",
        "        row = scaler.fit_transform(row)\n",
        "\n",
        "        dataset_norm[i, :] = np.reshape(row, (dataset.shape[1],))\n",
        "\n",
        "    return dataset_norm\n",
        "\n"
      ],
      "metadata": {
        "id": "KzyjDYqKgsBS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Everything Else"
      ],
      "metadata": {
        "id": "Wfs-mTJMg2mR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oYCK2gafSYe5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Read one week of data\n",
        "'''\n",
        "week = 1\n",
        "\n",
        "# Format the week with leading zeros\n",
        "number = f\"{week:02d}\"\n",
        "\n",
        "# Path of file\n",
        "#base_path = f\"data/X{number}/X{number}\"\n",
        "base_path = f\"X{number}\"\n",
        "\n",
        "# Load file\n",
        "data = np.loadtxt(base_path)\n",
        "\n",
        "# Keep first 144 entries from each line\n",
        "data = data[:, :144]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qe8ccwvhSYe7"
      },
      "outputs": [],
      "source": [
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "data_norm = normalize_matrix(scaler, data)\n",
        "\n",
        "# Train-Test Split\n",
        "train_data, test_data = train_test_split(data_norm, 0.8)\n",
        "\n",
        "# Window the dataset\n",
        "trainX, trainY = create_dataset(train_data, 10)\n",
        "testX, testY = create_dataset(test_data, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WkMGdlMiSYe7"
      },
      "outputs": [],
      "source": [
        "# Specify model parameters\n",
        "input_size = trainX.shape[2] # Number of features in input\n",
        "hidden_size = 200  # Number of features in hidden state\n",
        "output_size = 144  # Number of output classes\n",
        "learn_rate = 0.001\n",
        "epochs = 100\n",
        "num_layers = 1\n",
        "batch_size = 32\n",
        "shuffle = False #don't want to lose the time dependency\n",
        "num_workers = 2  # Number of subprocesses to use for data loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ramP2hSgYNDM",
        "outputId": "f4fd751b-621a-44f7-81ec-569f5c3f31bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-compute MLUs for ground truth training set to avoid for loop lag\n",
        "G = abilene_topo_lossfn()\n",
        "baseline_mlu = np.empty((trainY.shape[0]))\n",
        "\n",
        "for i in range(trainY.shape[0]):\n",
        "    target = trainY[i, :].reshape((12, 12))\n",
        "    np.fill_diagonal(target, 0)\n",
        "    u = MinMaxLinkUtil(G, target)\n",
        "    baseline_mlu[i] = u"
      ],
      "metadata": {
        "id": "Bfig2wzrFHJ4"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5nqBmVnTSYe7"
      },
      "outputs": [],
      "source": [
        "# create torch datasets and torch dataloader\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(trainX),\n",
        "                                                 torch.Tensor(trainY),\n",
        "                                               torch.FloatTensor(baseline_mlu))\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                             num_workers=num_workers,\n",
        "                                             shuffle=shuffle)\n",
        "\n",
        "test_dataset  = torch.utils.data.TensorDataset(torch.FloatTensor(testX),\n",
        "                                                 torch.Tensor(testY))\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1,\n",
        "                                             num_workers=num_workers,\n",
        "                                             shuffle=shuffle)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "H4_qvraESYe7"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.out = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape (batch, time_step, input_size)\n",
        "        # r_out shape (batch, time_step, output_size)\n",
        "        # h_n shape (n_layers, batch, hidden_size)\n",
        "        # h_c shape (n_layers, batch, hidden_size)\n",
        "        r_out, _ = self.rnn(x, None)  # None represents zero initial hidden state\n",
        "        out = self.out(r_out[:, -1, :])  # return the last value\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "import time\n",
        "\n",
        "# Define training function\n",
        "def train(model, train_loader, epochs, criterion, optimizer):\n",
        "    ''' Train ML Model'''\n",
        "\n",
        "    print_interval = 5\n",
        "    track_losses = np.zeros(epochs)\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets, mlus in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            mlus = mlus.to(device)\n",
        "\n",
        "            # Pass data to Model\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs) #size ->: batch_size x output_size\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, mlus) #size ->: float\n",
        "\n",
        "            # Compute the gradient and update the network parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        training_loss = loss.item()\n",
        "        track_losses[epoch] = training_loss\n",
        "\n",
        "        if (epoch) % (print_interval-1) == 0:\n",
        "            print('epoch: %4d training loss:%10.3e time:%7.1f'%(epoch, training_loss, time.time()-start))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-6R4EAMSYe7"
      },
      "source": [
        "The naive solution uses the output of the MLU and the ground truth MLU, computes the MLU, and performs back propogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "SEnSjYhnSYe8"
      },
      "outputs": [],
      "source": [
        "class NaiveLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, G, device):\n",
        "        super(NaiveLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.G = G\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # targets in this sense are the baseline mlus\n",
        "        inputs = inputs.reshape((-1, 12, 12))\n",
        "\n",
        "        # Use list comprehension to compute u_pred\n",
        "        u_pred_list = [MinMaxLinkUtil(self.G, inputs[i, :, :].fill_diagonal_(0)) for i in range(self.batch_size)]\n",
        "\n",
        "        # Convert list to tensor\n",
        "        u_pred = torch.tensor(u_pred_list).reshape(self.batch_size)\n",
        "\n",
        "        u_pred.requires_grad_()\n",
        "        targets.requires_grad_()\n",
        "        comp_loss = nn.MSELoss()\n",
        "\n",
        "        # compute the loss\n",
        "        return comp_loss(targets, u_pred.to(self.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVxjskEtSYe8"
      },
      "outputs": [],
      "source": [
        "# Create Model\n",
        "model = RNN(input_size, hidden_size, num_layers)\n",
        "\n",
        "# Move the model to the specified device\n",
        "model = model.to(device)\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "# Create loss function\n",
        "G = abilene_topo_lossfn()\n",
        "criterion = NaiveLoss(batch_size, G, device)\n",
        "\n",
        "loss = train(model, train_loader, epochs, criterion, optimizer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "MxbnFUemgwmF"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}